{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78656c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. (Uncomment to install) Install Hugging Face Transformers\n",
    "# !pip install transformers\n",
    "\n",
    "# 2. Import the core classes/functions\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoModelForCausalLM,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71622024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentiment]  Label: POSITIVE, Score: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Sentiment Analysis with a Pipeline (uses a cached model by default)\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "result = sentiment(\"movie is good\")[0]\n",
    "print(f\"[Sentiment]  Label: {result['label']}, Score: {result['score']:.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5431aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Downloading a Pretrained Model & Tokenizer Locally\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "#    - from_pretrained will download and cache to your ~/.cache/huggingface folder\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model     = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# 5. Save the downloaded files into your own folder:\n",
    "save_dir = \"./local_sentiment_model\"\n",
    "tokenizer.save_pretrained(save_dir)  # writes vocab & config\n",
    "model.save_pretrained(save_dir)      # writes model weights & config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2faa36ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9998651742935181}\n"
     ]
    }
   ],
   "source": [
    "# 6. Reloading from Your Local Copy (no Internet needed)\n",
    "local_tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "local_model     = AutoModelForSequenceClassification.from_pretrained(save_dir)\n",
    "\n",
    "#    - Create a new pipeline pointing at your local files\n",
    "local_sentiment = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=local_model,\n",
    "    tokenizer=local_tokenizer\n",
    ")\n",
    "print(local_sentiment(\"Kingdom movie is good!\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0245498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q&A]        Answer: state-of-the-art NLP, Score: 0.7716\n"
     ]
    }
   ],
   "source": [
    "# 7. Question Answering Pipeline\n",
    "qa = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\",\n",
    "    tokenizer=\"distilbert-base-cased-distilled-squad\"\n",
    ")\n",
    "context  = (\n",
    "    \"Hugging Face is a company that makes state-of-the-art NLP easy to use.\"\n",
    ")\n",
    "question = \"What does Hugging Face make easy?\"\n",
    "answer   = qa(question=question, context=context)\n",
    "print(f\"[Q&A]        Answer: {answer['answer']}, Score: {answer['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11bac811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fill-Mask]  Token: public, Score: 0.1053\n"
     ]
    }
   ],
   "source": [
    "# 8. Fill-Mask Pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "masked   = fill_mask(\"Transformers is a [MASK] library.\")[0]\n",
    "print(f\"[Fill-Mask]  Token: {masked['token_str']}, Score: {masked['score']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1089d26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Generation] Once upon a time! there lived a ghostly apparition, she turned. The ghost began the haunting and tried to awaken him. \"You are\n"
     ]
    }
   ],
   "source": [
    "# 9. Text Generation Pipeline\n",
    "text_gen = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "gen_out  = text_gen(\n",
    "    \"Once upon a time! there lived a ghost\",\n",
    "    max_length=30,          # total tokens (prompt + generated)\n",
    "    num_return_sequences=1  # how many variants\n",
    ")[0]\n",
    "print(f\"[Generation] {gen_out['generated_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uday_U01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
